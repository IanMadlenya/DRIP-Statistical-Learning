#DRIP Statistical Learning

**v2.34**  *2 October 2016*

DRIP Statistical Learning is a collection of Java libraries for Machine Learning and Statistical Evaluation.

DRIP Statistical Learning is composed of the following main libraries:
 * Probabilistic Sequence Measure Concentration Bounds Library
 * Statistical Learning Theory Framework Library
 * Empirical Risk Minimization Library
 * VC and Capacity Measure Library

For Installation, Documentation and Samples, and the associated supporting Numerical Libraries please check out [DRIP] (https://github.com/lakshmiDRIP/DRIP).


##Features

###Probabilistic Bounds and Concentration of Measure Sequences
####Probabilistic Bounds
 * Tail Probability Bounds Estimation
 * Basic Probability Inequalities
 * Cauchy-Schwartz Inequality
 * Association Inequalities
 * Moment, Gaussian, and Exponential Bounds
 * Bounding Sums of Independent Random Variables
 * Non Moment Based Bounding - Hoeffding Bound
 * Moment Based Bounds
 * Binomial Tails
 * Custom Bounds for Special i.i.d. Sequences

####Efron Stein Bounds
 * Martingale Differences Sum Inequality
 * Efron-Stein Inequality
 * Bounded Differences Inequality
 * Bounded Differences Inequality - Applications
 * Self-Bounding Functions
 * Configuration Functions
 
####Entropy Methods
 * Information Theory - Basics
 * Tensorization of the Entropy
 * Logarithmic Sobolev Inequalities
 * Logarithmic Sobolev Inequalities - Applications
 * Exponential Inequalities for Self-Bounding Functions
 * Combinatorial Entropy
 * Variations on the Theme of Self-Bounding Functions

####Concentration of Measure
 * Equivalent Bounded Differences Inequality
 * Convex Distance Inequality
 * Convex Distance Inequality - Proof
 * Application of the Convex Distance Inequality - Bin Packing

###Statistical Learning Theory - Foundation and Framework
####Standard SLT Framework
 * Computational Learning Theory
 * Probably Approximately Correct (PAC) Learning
 * PAC Definitions and Terminology
 * SLT Setup
 * Algorithms for Reducing Over-fitting
 * Bayesian Normalized Regularizer Setup

####Generalization and Consistency
 * Types of Consistency
 * Bias-Variance or Estimation-Approximation Trade-off
 * Bias-Variance Decomposition
 * Bias-Variance Optimization
 * Generalization and Consistency for kNN

###Empirical Risk Minimization - Principles and Techniques
####Empirical Risk Minimization
 * Overview
 * The Loss Functions and Empirical Risk Minimization Principles
 * Application of the Central Limit Theorem (CLT) and the Law of Large Numbers (LLN)
 * Inconsistency of Empirical Risk Minimizers
 * Uniform Convergence
 * ERM Complexity

####Symmetrization
 * The Symmetrization Lemma

####Generalization Bounds
 * The Union Bound
 * Shattering Coefficient
 * Empirical Risk Generalization Bound
 * Large Margin Bounds

####Rademacher Complexity
 * Rademacher-based Uniform Convergence
 * VC Entropy
 * Chaining Technique

####Local Rademacher Averages
 * Star-Hull and Sub-Root Functions
 * Local Rademacher Averages and Fixed Point
 * Local Rademacher Averages - Consequences

####Normalized ERM
 * Computing the Normalized Empirical Risk Bounds
 * De-normalized Bounds

####Noise Conditions
 * SLT Analysis Metrics
 * Types of Noise Conditions
 * Relative Loss Class

###VC Theory and Capacity Measure Analysis
####VC Theory and VC Dimension
 * Empirical Processes
 * Bounding the Empirical Loss Function
 * VC Dimension - Setup
 * Incorporating the Formal VC Definition
 * VC Dimension Examples
 * VC Dimension vs. Popper's Dimension

####Sauer Lemma and VC Classifier Framework
 * Working out Sauer Lemma Bounds
 * Sauer Lemma ERM Bounds
 * VC Index
 * VC Classifier Framework

##Contact

lakshmi@synergicdesign.com
